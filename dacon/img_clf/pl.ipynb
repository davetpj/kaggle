{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pl.ipynb","provenance":[],"authorship_tag":"ABX9TyOG205LWnhkXlv4HonHMFi2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Yf66mf1xfLp4"},"source":["import torch\n","import pytorch_lightning as pl\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, random_split\n","\n","from torchvision.datasets.mnist import MNIST\n","from torchvision import transforms\n","\n","\n","class Backbone(torch.nn.Module):\n","    def __init__(self, hidden_dim=128):\n","        super().__init__()\n","        self.l1 = torch.nn.Linear(28 * 28, hidden_dim)\n","        self.l2 = torch.nn.Linear(hidden_dim, 10)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.l1(x))\n","        x = torch.relu(self.l2(x))\n","        return x\n","\n","\n","class LitClassifier(pl.LightningModule):\n","    def __init__(self, backbone, learning_rate=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.backbone = backbone\n","\n","    def forward(self, x):\n","        # use forward for inference/predictions\n","        embedding = self.backbone(x)\n","        return embedding\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.backbone(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        self.log('train_loss', loss, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.backbone(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        self.log('valid_loss', loss, on_step=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self.backbone(x)\n","        loss = F.cross_entropy(y_hat, y)\n","        self.log('test_loss', loss)\n","\n","    def configure_optimizers(self):\n","        # self.hparams available because we called self.save_hyperparameters()\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n","\n","    @staticmethod\n","    def add_model_specific_args(parent_parser):\n","        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n","        parser.add_argument('--learning_rate', type=float, default=0.0001)\n","        return parser\n","\n","\n","def cli_main():\n","    pl.seed_everything(1234)\n","\n","    # ------------\n","    # args\n","    # ------------\n","    parser = ArgumentParser()\n","    parser.add_argument('--batch_size', default=32, type=int)\n","    parser.add_argument('--hidden_dim', type=int, default=128)\n","    parser = pl.Trainer.add_argparse_args(parser)\n","    parser = LitClassifier.add_model_specific_args(parser)\n","    args = parser.parse_args()\n","\n","    # ------------\n","    # data\n","    # ------------\n","    dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n","    mnist_test = MNIST('', train=False, download=True, transform=transforms.ToTensor())\n","    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n","\n","    train_loader = DataLoader(mnist_train, batch_size=args.batch_size)\n","    val_loader = DataLoader(mnist_val, batch_size=args.batch_size)\n","    test_loader = DataLoader(mnist_test, batch_size=args.batch_size)\n","\n","    # ------------\n","    # model\n","    # ------------\n","    model = LitClassifier(Backbone(hidden_dim=args.hidden_dim), args.learning_rate)\n","\n","    # ------------\n","    # training\n","    # ------------\n","    trainer = pl.Trainer.from_argparse_args(args)\n","    trainer.fit(model, train_loader, val_loader)\n","\n","    # ------------\n","    # testing\n","    # ------------\n","    result = trainer.test(test_dataloaders=test_loader)\n","    print(result)"],"execution_count":null,"outputs":[]}]}